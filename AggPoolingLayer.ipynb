{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27515552-5f93-4bd9-951f-7363be5d680a",
   "metadata": {},
   "source": [
    "# Estudio del impacto del reemplazo de la función de pooling en una CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15be39b0-c61d-40d6-9591-82a8a6960a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  # Commonly used functions like relu, softmax\n",
    "from torch.nn import Unfold \n",
    "\n",
    "# Data loading\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Auxiliary functions\n",
    "from torch.utils.tensorboard import SummaryWriter  # Used for Tensorboard logging\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import floor, ceil\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d914a71-05b1-4dad-8b86-bc4697ae87ea",
   "metadata": {},
   "source": [
    "# Directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e4c4d31-da00-4606-86bd-2471a4d6fd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructura de carpetas creada:\n",
      "Data directory: cnn_project/data\n",
      "Reports directory: cnn_project/reports\n",
      "Models directory: cnn_project/reports/models\n",
      "Results directory: cnn_project/reports/results\n",
      "Runs directory: cnn_project/reports/runs\n"
     ]
    }
   ],
   "source": [
    "# Configuración de directorios para la organización del proyecto\n",
    "\n",
    "# Directorio raíz del proyecto\n",
    "base_dir = 'cnn_project'\n",
    "\n",
    "# Subdirectorios\n",
    "data_dir = os.path.join(base_dir, 'data')                 # Para datasets\n",
    "reports_dir = os.path.join(base_dir, 'reports')            # Para resultados y modelos\n",
    "models_dir = os.path.join(reports_dir, 'models')           # Para guardar modelos entrenados\n",
    "results_dir = os.path.join(reports_dir, 'results')         # Para los resultados de pruebas en el conjunto de test\n",
    "runs_dir = os.path.join(reports_dir, 'runs')               # Para logs de Tensorboard\n",
    "\n",
    "# Crear los directorios si no existen\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(runs_dir, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir=runs_dir)\n",
    "\n",
    "print(\"Estructura de carpetas creada:\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Reports directory: {reports_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "print(f\"Runs directory: {runs_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c8443-e4ae-4cea-918c-97777ae6f56d",
   "metadata": {},
   "source": [
    "## Cuda o CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d47d964-4d10-4dad-a62e-dc9f53e63768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2731cec-88a5-47e0-8819-a23388f95bd2",
   "metadata": {},
   "source": [
    "## Carga de datos: Datasets y Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5031c5e9-02fb-4a2e-aa9c-ee88c8cd6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proporción de entrenamiento\n",
    "train_proportion = 0.9  \n",
    "num_train = 5000\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(train_proportion * num_train))\n",
    "\n",
    "np.random.shuffle(indices) \n",
    "\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "# Generar samplers para el conjunto de entrenamiento y validación\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "adef36b1-7fde-44ae-9890-77a4bc0859c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers =  2\n",
    "KERNEL_SIZE = 3\n",
    "STRIDE = 3\n",
    "PADDING = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56c56e74-6417-46c5-bd10-6dc92cb4f603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Datasets cargados y dataloaders creados.\n",
      "Tamaño del conjunto de entrenamiento: 4500\n",
      "Tamaño del conjunto de validación: 500\n",
      "Tamaño del conjunto de prueba: 10000\n"
     ]
    }
   ],
   "source": [
    "# Transformaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización para CIFAR-10\n",
    "])\n",
    "\n",
    "# Descarga del dataset CIFAR-10\n",
    "train_dataset = datasets.CIFAR10(root=data_dir, train=True, transform=transform, download=True)\n",
    "val_dataset = datasets.CIFAR10(root=data_dir, train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.CIFAR10(root=data_dir, train=False, transform=transform, download=True)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(\"Datasets cargados y dataloaders creados.\")\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {len(train_loader.sampler)}\")\n",
    "print(f\"Tamaño del conjunto de validación: {len(val_loader.sampler)}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af9e839-8b05-455f-b80d-4f344b006bd6",
   "metadata": {},
   "source": [
    "## AggPoolingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2bc8bd43-a250-4e12-89b2-fe0c8a856f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggPoolingLayer(nn.Module):\n",
    "    def __init__(self, kernel_size, stride=None, padding=0, agg_function='mean', keepdims=False):\n",
    "        super(AggPoolingLayer, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride or kernel_size\n",
    "        self.padding = padding\n",
    "        self.agg_function = agg_function\n",
    "        self.keepdims = keepdims\n",
    "        self.owa_weights = nn.Parameter(torch.ones(self.kernel_size * self.kernel_size))  # Define con el tamaño del bloque\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aplicar im2col para dividir el tensor en bloques\n",
    "        unfolded_x = F.unfold(x, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "        unfolded_x = unfolded_x.view(x.size(0), x.size(1), self.kernel_size * self.kernel_size, -1)\n",
    "\n",
    "        # Normalización min-max\n",
    "        min_vals = unfolded_x.min(dim=2, keepdim=True)[0]\n",
    "        max_vals = unfolded_x.max(dim=2, keepdim=True)[0]\n",
    "        unfolded_x = (unfolded_x - min_vals) / (max_vals - min_vals + 1e-6)\n",
    "        \n",
    "        # Selección de la función de agregación\n",
    "        if self.agg_function == 'mean':\n",
    "            agg_result = unfolded_x.mean(dim=2)\n",
    "        elif self.agg_function == 'max':\n",
    "            agg_result = unfolded_x.max(dim=2)[0]\n",
    "        elif self.agg_function == 'owa':\n",
    "            # Ordenar los valores en cada bloque y aplicar los pesos de OWA\n",
    "            sorted_vals, _ = unfolded_x.sort(dim=2, descending=True)\n",
    "            # Asegura que `owa_weights` tenga el mismo tamaño que cada bloque\n",
    "            owa_weights_expanded = self.owa_weights.view(1, 1, -1, 1)\n",
    "            agg_result = (sorted_vals * owa_weights_expanded).sum(dim=2)\n",
    "        elif self.agg_function == 'choquet':\n",
    "            sorted_vals, _ = unfolded_x.sort(dim=2, descending=True)\n",
    "            choquet_vals = sorted_vals * torch.cumsum(sorted_vals, dim=2)\n",
    "            agg_result = choquet_vals.sum(dim=2)\n",
    "        elif self.agg_function == 'tnorm_min':\n",
    "            agg_result = unfolded_x.min(dim=2)[0]\n",
    "        elif self.agg_function == 'tnorm_product':\n",
    "            agg_result = unfolded_x.prod(dim=2)\n",
    "        elif self.agg_function == 'tnorm_lukasiewicz':\n",
    "            agg_result = torch.clamp(unfolded_x.sum(dim=2) - 1, min=0)\n",
    "        elif self.agg_function == 'tnorm_hamacher':\n",
    "            agg_result = (unfolded_x.prod(dim=2)) / (unfolded_x.sum(dim=2) - unfolded_x.prod(dim=2) + 1e-6)\n",
    "        elif self.agg_function == 'tconorm_max':\n",
    "            agg_result = unfolded_x.max(dim=2)[0]\n",
    "        elif self.agg_function == 'tconorm_lukasiewicz':\n",
    "            agg_result = torch.clamp(unfolded_x.sum(dim=2), max=1)\n",
    "        elif self.agg_function == 'tconorm_hamacher':\n",
    "            agg_result = (unfolded_x.sum(dim=2) - unfolded_x.prod(dim=2)) / (1 - unfolded_x.prod(dim=2) + 1e-6)\n",
    "        elif self.agg_function == 'uninorm_minmax':\n",
    "            agg_result = torch.where(unfolded_x <= 0.5, unfolded_x.min(dim=2)[0], unfolded_x.max(dim=2)[0])\n",
    "        elif self.agg_function == 'uninorm_ll':\n",
    "            agg_result = torch.where(unfolded_x <= 0.5, torch.clamp(unfolded_x.sum(dim=2) - 1, min=0), torch.clamp(unfolded_x.sum(dim=2), max=1))\n",
    "        else:\n",
    "            raise ValueError(f\"Función de agregación desconocida: {self.agg_function}\")\n",
    "\n",
    "        # Desnormalización\n",
    "        agg_result = agg_result * (max_vals.squeeze(2) - min_vals.squeeze(2) + 1e-6) + min_vals.squeeze(2)\n",
    "        \n",
    "        # Cambiar la forma a [batch_size, num_channels, output_height, output_width]\n",
    "        output_size = int((x.size(2) + 2 * self.padding - self.kernel_size) / self.stride + 1)\n",
    "        agg_result = agg_result.view(x.size(0), x.size(1), output_size, output_size)\n",
    "        \n",
    "        return agg_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af088b11-6684-4c25-89c3-d85dc53e3e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamacher_tconorm(tensor, keepdim=False, dim=-1):\n",
    "    tensor_shape = list(tensor.shape)\n",
    "    tensor_shape.pop(dim)\n",
    "    out_tensor = tensor.new_zeros(tensor_shape)\n",
    "    ones = tensor.new_ones(tensor_shape)\n",
    "    # Indexar la Ãºtlima dimensiÃ³n facilita la legibilidad del cÃ³digo (tendrÃ­amos que usar torch.index_select en caso contrario)\n",
    "    if (dim == -1) or (dim == len(tensor.shape)-1):\n",
    "        out_tensor = tensor[..., 0] # Tensor auxiliar donde acumularemos la salida (harÃ¡ las veces de x)\n",
    "        for i in range(1, tensor.shape[dim]):  # La t-conorma es asociativa: Trataremos los elementos de 2 en 2\n",
    "            # Dado que la t-conorma es asociativa, trataremos los elementos de 2 en 2. En cada iteraciÃ³n:\n",
    "            x = out_tensor\n",
    "            y = tensor[..., i]\n",
    "            diff_indices = torch.where(torch.abs(torch.mul(x, y)-1) > 1e-9) # Devuelve los Ã­ndices de los elementos para los cuÃ¡les x*y-1 > 0 (condiciÃ³n de la funciÃ³n por partes)\n",
    "            # Asignamos los valores en funciÃ³n de las condiciones\n",
    "            # if ab == 1 -> T(a, b) = 1\n",
    "            out_tensor = ones  # Por defecto, asumimos que todos los valores caen en el caso x*y-1=0\n",
    "            # otherwise -> T(a, b) = (2ab - a - b) / (ab - 1)\n",
    "            out_tensor[diff_indices] = (\n",
    "                2 * torch.mul(x[diff_indices], y[diff_indices]) - x[diff_indices] - y[diff_indices]) / (\n",
    "                torch.mul(x[diff_indices], y[diff_indices]) - 1)  # Corregimos los valores para los cuÃ¡les x*y-1>0 (los que corresponden a los Ã­ndices de diff_indices)\n",
    "    else:\n",
    "        # El cÃ³digo serÃ­a idÃ©ntico, sustituyendo tensor[..., 0] por torch.index_select(tensor, dim, tensor.new_tensor([0], dtype=torch.int)).squeeze(dim)\n",
    "        # torch.index_select(tensor, dim, tensor.new_tensor([0], dtype=torch.int)).squeeze(dim) indexa todos los elementos de la dimensiÃ³n dim\n",
    "        # NO HACE FALTA IMPLEMENTARLO\n",
    "        raise Exception('Utilizar la versiÃ³n con dim=-1')\n",
    "    if keepdim:\n",
    "        torch.unsqueeze(out_tensor, dim=dim)\n",
    "    return out_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb71798-1575-4edf-a30b-705d346ba71d",
   "metadata": {},
   "source": [
    "## Definición del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "43a9770e-af1b-4af1-92bf-51e8f4dbf8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = AggPoolingLayer(kernel_size=3, stride=1, padding=0, agg_function='mean')\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = AggPoolingLayer(kernel_size=3, stride=1, padding=0, agg_function='mean')\n",
    "        \n",
    "        # Calcular el tamaño de la salida después de las capas de convolución y pooling\n",
    "        self._to_linear = None\n",
    "        self.convs(torch.randn(1, 3, 32, 32))\n",
    "        \n",
    "        # Definir capas totalmente conectadas\n",
    "        self.fc = nn.Linear(self._to_linear, 128)  # Capa intermedia\n",
    "        self.fc2 = nn.Linear(128, 10)              # Capa final para 10 clases de salida\n",
    "        \n",
    "    def convs(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x.view(x.size(0), -1).shape[1]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)         # Pasa por las capas convolucionales y de pooling\n",
    "        x = x.view(x.size(0), -1) # Aplana el tensor antes de la capa totalmente conectada\n",
    "        x = self.fc(x)            # Primera capa totalmente conectada\n",
    "        x = F.relu(x)             # Activación ReLU para la primera capa totalmente conectada\n",
    "        x = self.fc2(x)           # Segunda capa totalmente conectada para la salida final\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e70332-33da-4fb8-8258-d4538fd1037f",
   "metadata": {},
   "source": [
    "## Configuración de TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "da355ad5-aeee-49be-b77a-97a632b41110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, writer):\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Poner el modelo en modo de entrenamiento\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Reiniciar los gradientes\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Paso hacia adelante\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Paso hacia atrás y optimización\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Acumular la pérdida y la precisión\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calcular la pérdida y precisión de la época\n",
    "        epoch_loss = train_loss / len(train_loader)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "\n",
    "        # Registrar en TensorBoard\n",
    "        writer.add_scalar('Pérdida de entrenamiento', epoch_loss, epoch)\n",
    "        writer.add_scalar('Precisión de entrenamiento', epoch_accuracy, epoch)\n",
    "\n",
    "        # Validación del modelo\n",
    "        validate_model(model, val_loader, criterion, epoch, writer)\n",
    "\n",
    "        print(f'Epoca [{epoch + 1}/{num_epochs}], Pérdida: {epoch_loss:.4f}, Precisión: {epoch_accuracy:.2f}%')\n",
    "\n",
    "    return model\n",
    "\n",
    "def validate_model(model, val_loader, criterion, epoch, writer):\n",
    "    model.eval()  # Poner el modelo en modo de evaluación\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No calcular gradientes en validación\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Acumular la pérdida y la precisión\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calcular la pérdida y precisión de validación\n",
    "    epoch_val_loss = val_loss / len(val_loader)\n",
    "    epoch_val_accuracy = 100 * correct / total\n",
    "\n",
    "    # Registrar en TensorBoard\n",
    "    writer.add_scalar('Pérdida de validación', epoch_val_loss, epoch)\n",
    "    writer.add_scalar('Precisión de validación', epoch_val_accuracy, epoch)\n",
    "\n",
    "    print(f'Validación - Pérdida: {epoch_val_loss:.4f}, Precisión: {epoch_val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a15fc-583f-4c40-8d7b-5f8ee94dcc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validación - Pérdida: 2.2791, Precisión: 22.20%\n",
      "Epoca [1/20], Pérdida: 2.2952, Precisión: 13.67%\n",
      "Validación - Pérdida: 2.2280, Precisión: 24.60%\n",
      "Epoca [2/20], Pérdida: 2.2577, Precisión: 23.91%\n",
      "Validación - Pérdida: 2.1456, Precisión: 27.20%\n",
      "Epoca [3/20], Pérdida: 2.1876, Precisión: 25.13%\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN()\n",
    "\n",
    "# Inicialización de los parámetros\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "# Entrena el modelo\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, writer)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac5ca1-4be4-4db0-a84c-4c8958950623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
